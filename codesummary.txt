âœ… 1. What is this code for?
This Python script predicts Telco customer churn using an AdaBoost classifier.
It:

Trains a model using a real-world customer dataset (Telco Churn dataset)

Evaluates the model with key classification metrics

Saves the trained model and scaler

Accepts new user input line-by-line and makes a churn prediction

The goal: Identify customers likely to leave (churn) so the company can take proactive retention steps.

ğŸ¢ 2. List of companies that use such churn prediction systems
Companies that benefit from churn prediction:

Telecommunication companies â€“ e.g., Verizon, AT&T, Jio, Airtel

SaaS providers â€“ e.g., Salesforce, Zoom, Microsoft 365

Streaming platforms â€“ e.g., Netflix, Spotify, Disney+

ISPs and Cable TV providers

Banks and insurance firms â€“ for customer attrition prediction

E-commerce platforms â€“ predicting loss of loyal buyers

ğŸ’¡ 3. Why did we use AdaBoost?
AdaBoost (Adaptive Boosting) is used because:

It boosts weak learners (like decision stumps) into a strong ensemble

Performs well on imbalanced data (like churn datasets where 'No' dominates)

Handles both categorical and numerical data

Offers better generalization and avoids overfitting compared to a single tree

Also, DecisionTreeClassifier(max_depth=1) creates a decision stump (weak learner), which is ideal for AdaBoost.

ğŸ“¥ 4. List of all inputs asked (features) and their meaning + example
Hereâ€™s what each input means (you enter these line-by-line in the final part of the code):

Feature	Meaning	Example Input
gender	Customer's gender (0 = Female, 1 = Male)	1
Partner	Has a partner? (1 = Yes, 0 = No)	1
Dependents	Has dependents? (1 = Yes, 0 = No)	0
PhoneService	Subscribed to phone service?	1
PaperlessBilling	Uses paperless billing?	1
tenure	Number of months with the company	12
MonthlyCharges	Monthly billing amount	70.55
TotalCharges	Total amount billed over tenure	800.50
All others like InternetService_Fiber optic	One-hot encoded dummy variables for categorical fields	1 (if active), else 0

If you don't want to enter a value, type noinput, and it will default to 0.

âš™ï¸ 5. All hyperparameters and why they are used
ğŸ” Train-test split:
python
Copy
Edit
train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
test_size=0.2: 20% for testing, a common split for balanced evaluation

stratify=y: Ensures class balance is maintained in both train and test sets

random_state=42: Ensures reproducibility of results

ğŸ“Š AdaBoost hyperparameters:
python
Copy
Edit
AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=50,
    learning_rate=1.0,
    random_state=42
)
Parameter	Value	Reason
estimator	DecisionTreeClassifier(max_depth=1)	A decision stump, which is a weak learner ideal for boosting
n_estimators	50	Boosts 50 stumps; balances performance and overfitting
learning_rate	1.0	Controls contribution of each learner; 1.0 is standard for AdaBoost
random_state	42	Reproducibility

ğŸ“ Scaling with StandardScaler:
python
Copy
Edit
scaler = StandardScaler()
Standardizes features (mean = 0, std = 1). Important for distance-based models and helps speed up convergence.

ğŸ“‰ Evaluation & Plot Hyperparameters:
Confusion Matrix plot
python
Copy
Edit
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
figsize=(6, 4): Decent resolution for clear visuals

annot=True, fmt='d': Shows integer values in cells

ROC Curve
python
Copy
Edit
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_test, y_proba):.2f}")
plt.plot([0, 1], [0, 1], 'k--')
Plots the true positive rate vs false positive rate

The diagonal k-- is the baseline (random guessing)

ğŸŒŸ Feature Importance Plot:
python
Copy
Edit
feat_df.sort_values(by='Importance', ascending=False).head(15)
plt.figure(figsize=(10, 6))
Shows top 15 most important features according to AdaBoost

Larger figsize for better layout and readability
